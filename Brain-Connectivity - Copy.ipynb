{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import scipy.io\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import mne\n",
    "# import copy\n",
    "# import logging\n",
    "# import mne_connectivity\n",
    "# import networkx as nx\n",
    "# from statsmodels.tsa.stattools import grangercausalitytests\n",
    "# from pyinform.mutualinfo import mutual_info\n",
    "# from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from scipy.signal import coherence\n",
    "# from scipy.signal import csd, welch\n",
    "# from matplotlib import cm\n",
    "from scipy.signal import hilbert\n",
    "# from matplotlib.colors import LogNorm\n",
    "from mne_connectivity import spectral_connectivity_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Matrix Depression Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_fif_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.fif')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_FT7_depression\\alpha_band_FT7_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_T7_depression\\alpha_band_T7_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_TP7_depression\\alpha_band_TP7_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_TP8_depression\\alpha_band_TP8_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_T8_depression\\alpha_band_T8_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_FT8_depression\\alpha_band_FT8_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_F3_depression\\alpha_band_F3_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_F4_depression\\alpha_band_F4_depression\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['FT7', 'T7', 'TP7', 'TP8', 'T8', 'FT8', 'F3', 'F4']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_fif_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .fif files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\coherence_visualization_depression\\alpha_band_coherence_depression\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\csv_data_depression\\alpha_band_csv_depression\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=8, fmax=12, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "            \n",
    "            csv_file_name = f\"coherence_matrix_Depression_alphaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Alpha Band Depression')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Depression_alphaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha Band (dataset real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_edf_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.edf')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_C3-R_depression\\alpha_band_C3-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_C4-R_depression\\alpha_band_C4-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_FP1-R_depression\\alpha_band_FP1-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_FP2-R_depression\\alpha_band_FP2-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_O1-R_depression\\alpha_band_O1-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_O2-R_depression\\alpha_band_O2-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_T3-R_depression\\alpha_band_T3-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_T4-R_depression\\alpha_band_T4-R_depression\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['EEG FP1-R', 'EEG FP2-R', 'EEG C3-R', 'EEG C4-R', 'EEG O1-R', 'EEG O2-R', 'EEG T3-R', 'EEG T4-R']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_edf_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .edf files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\coherence_visualization_depression\\alpha_band_coherence_depression\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\csv_data_coherence_depression\\alpha_band_csv_depression\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=8, fmax=12, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "            \n",
    "            csv_file_name = f\"coherence_matrix_Depression_alphaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Alpha Band Depression')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Depression_alphaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_fif_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.fif')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_FT7_depression\\beta_band_FT7_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_T7_depression\\beta_band_T7_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_TP7_depression\\beta_band_TP7_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_TP8_depression\\beta_band_TP8_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_T8_depression\\beta_band_T8_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_FT8_depression\\beta_band_FT8_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_F3_depression\\beta_band_F3_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_F4_depression\\beta_band_F4_depression\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['FT7', 'T7', 'TP7', 'TP8', 'T8', 'FT8', 'F3', 'F4']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_fif_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .fif files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\coherence_visualization_depression\\beta_band_coherence_depression\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\csv_data_depression\\beta_band_csv_depression\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=13, fmax=30, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "            \n",
    "            csv_file_name = f\"coherence_matrix_Depression_betaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Beta Band Depression')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Depression_betaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta band (dataset real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_edf_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.edf')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_C3-R_depression\\beta_band_C3-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_C4-R_depression\\beta_band_C4-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_FP1-R_depression\\beta_band_FP1-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_FP2-R_depression\\beta_band_FP2-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_O1-R_depression\\beta_band_O1-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_O2-R_depression\\beta_band_O2-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_T3-R_depression\\beta_band_T3-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_T4-R_depression\\beta_band_T4-R_depression\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['EEG FP1-R', 'EEG FP2-R', 'EEG C3-R', 'EEG C4-R', 'EEG O1-R', 'EEG O2-R', 'EEG T3-R', 'EEG T4-R']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_edf_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .edf files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\coherence_visualization_depression\\beta_band_coherence_depression\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\csv_data_coherence_depression\\beta_band_csv_depression\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=13, fmax=30, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "            \n",
    "            csv_file_name = f\"coherence_matrix_Depression_betaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Beta Band Depression')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Depression_betaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_fif_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.fif')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_FT7_depression\\delta_band_FT7_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_T7_depression\\delta_band_T7_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_TP7_depression\\delta_band_TP7_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_TP8_depression\\delta_band_TP8_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_T8_depression\\delta_band_T8_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_FT8_depression\\delta_band_FT8_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_F3_depression\\delta_band_F3_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_F4_depression\\delta_band_F4_depression\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['FT7', 'T7', 'TP7', 'TP8', 'T8', 'FT8', 'F3', 'F4']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_fif_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .fif files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\coherence_visualization_depression\\delta_band_coherence_depression\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\csv_data_depression\\delta_band_csv_depression\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=0.5, fmax=4, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "            \n",
    "            csv_file_name = f\"coherence_matrix_Depression_deltaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Delta Band Depression')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Depression_deltaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta band (dataset real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_edf_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.edf')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "       r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_C3-R_depression\\delta_band_C3-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_C4-R_depression\\delta_band_C4-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_FP1-R_depression\\delta_band_FP1-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_FP2-R_depression\\delta_band_FP2-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_O1-R_depression\\delta_band_O1-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_O2-R_depression\\delta_band_O2-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_T3-R_depression\\delta_band_T3-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_T4-R_depression\\delta_band_T4-R_depression\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['EEG FP1-R', 'EEG FP2-R', 'EEG C3-R', 'EEG C4-R', 'EEG O1-R', 'EEG O2-R', 'EEG T3-R', 'EEG T4-R']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_edf_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .edf files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\coherence_visualization_depression\\delta_band_coherence_depression\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\csv_data_coherence_depression\\delta_band_csv_depression\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=0.5, fmax=4, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "            \n",
    "            csv_file_name = f\"coherence_matrix_Depression_deltaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Delta Band Depression')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Depression_deltaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_fif_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.fif')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_FT7_depression\\gamma_band_FT7_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_T7_depression\\gamma_band_T7_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_TP7_depression\\gamma_band_TP7_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_TP8_depression\\gamma_band_TP8_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_T8_depression\\gamma_band_T8_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_FT8_depression\\gamma_band_FT8_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_F3_depression\\gamma_band_F3_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_F4_depression\\gamma_band_F4_depression\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['FT7', 'T7', 'TP7', 'TP8', 'T8', 'FT8', 'F3', 'F4']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_fif_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .fif files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\coherence_visualization_depression\\gamma_band_coherence_depression\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\csv_data_depression\\gamma_band_csv_depression\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=30, fmax=40, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "            \n",
    "            csv_file_name = f\"coherence_matrix_Depression_gammaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Gamma Band Depression')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Depression_gammaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma banda (dataset real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_edf_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.edf')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_C3-R_depression\\delta_band_C3-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_C4-R_depression\\delta_band_C4-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_FP1-R_depression\\delta_band_FP1-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_FP2-R_depression\\delta_band_FP2-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_O1-R_depression\\delta_band_O1-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_O2-R_depression\\delta_band_O2-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_T3-R_depression\\delta_band_T3-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_T4-R_depression\\delta_band_T4-R_depression\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['EEG FP1-R', 'EEG FP2-R', 'EEG C3-R', 'EEG C4-R', 'EEG O1-R', 'EEG O2-R', 'EEG T3-R', 'EEG T4-R']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_edf_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .edf files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\coherence_visualization_depression\\gamma_band_coherence_depression\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\csv_data_coherence_depression\\gamma_band_csv_depression\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=30, fmax=40, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "            \n",
    "            csv_file_name = f\"coherence_matrix_Depression_gammaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Gamma Band Depression')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Depression_gammaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theta Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_fif_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.fif')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_FT7_depression\\theta_band_FT7_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_T7_depression\\theta_band_T7_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_TP7_depression\\theta_band_TP7_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_TP8_depression\\theta_band_TP8_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_T8_depression\\theta_band_T8_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_FT8_depression\\theta_band_FT8_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_F3_depression\\theta_band_F3_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\filter_channel_F4_depression\\theta_band_F4_depression\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['FT7', 'T7', 'TP7', 'TP8', 'T8', 'FT8', 'F3', 'F4']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_fif_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .fif files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        print(info)\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        print(events)\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        data_epochs_combined_0 = epochs_combined.get_data()[0]\n",
    "        print(data_epochs_combined_0.shape)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\coherence_visualization_depression\\theta_band_coherence_depression\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\csv_data_depression\\theta_band_csv_depression\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=4, fmax=8, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_data = con.get_data()\n",
    "            print(con_data)\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            print(df_con)\n",
    "            processed_epochs_count += 1\n",
    "            \n",
    "            csv_file_name = f\"coherence_matrix_Depression_thetaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Theta Band Depression')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Depression_thetaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theta band (dataset real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_edf_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.edf')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_C3-R_depression\\delta_band_C3-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_C4-R_depression\\delta_band_C4-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_FP1-R_depression\\delta_band_FP1-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_FP2-R_depression\\delta_band_FP2-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_O1-R_depression\\delta_band_O1-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_O2-R_depression\\delta_band_O2-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_T3-R_depression\\delta_band_T3-R_depression\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\filter_channel_T4-R_depression\\delta_band_T4-R_depression\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['EEG FP1-R', 'EEG FP2-R', 'EEG C3-R', 'EEG C4-R', 'EEG O1-R', 'EEG O2-R', 'EEG T3-R', 'EEG T4-R']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_edf_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .edf files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\coherence_visualization_depression\\theta_band_coherence_depression\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\csv_data_coherence_depression\\theta_band_csv_depression\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=4, fmax=8, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "            \n",
    "            csv_file_name = f\"coherence_matrix_Depression_thetaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Theta Band Depression')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Depression_thetaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix \n",
    "## Dataset depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metode Korelasi Pearson dataset healthy\n",
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "directory_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\6s_epochs_depression\"\n",
    "selected_electrodes_names = ['FT7', 'T7', 'TP7', 'TP8', 'T8', 'FT8', 'F3', 'F4']\n",
    "file_paths = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith(\".fif\")]\n",
    "save_directory = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\correlation_visualization_depression\"\n",
    "save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Depression\\csv_data_correlation_depression\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "os.makedirs(save_csv_path, exist_ok=True) \n",
    "total_epochs = 0\n",
    "for file_path in file_paths:\n",
    "    epochs = mne.read_epochs(file_path, preload=True)\n",
    "    num_epochs = len(epochs)\n",
    "    total_epochs += num_epochs\n",
    "    file_basename = os.path.basename(file_path).split('.')[0]\n",
    "    print(f\"{os.path.basename(file_path)}: {num_epochs} epochs\")\n",
    "\n",
    "    for epoch_idx, epoch_data in enumerate(epochs.get_data()):\n",
    "        selected_indices = [epochs.ch_names.index(name) for name in selected_electrodes_names if name in epochs.ch_names]\n",
    "        selected_data = epoch_data[selected_indices]\n",
    "        column_labels = [epochs.ch_names[idx] for idx in selected_indices]\n",
    "        df = pd.DataFrame(selected_data.T, columns=column_labels)\n",
    "        correlation_matrix = df.corr()\n",
    "        \n",
    "        csv_filename = f\"{file_basename}_correlation_matrix_epoch_{epoch_idx + 1}.csv\"\n",
    "        csv_file_path = os.path.join(save_csv_path, csv_filename)\n",
    "        correlation_matrix.to_csv(csv_file_path)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap=\"viridis\", fmt=\".2f\", \n",
    "                    xticklabels=column_labels, \n",
    "                    yticklabels=column_labels)\n",
    "        file_name = f\"{file_basename}_correlation_matrix_epoch_{epoch_idx + 1}.png\"\n",
    "        plt.title(f\"Pearson Correlation Matrix\")\n",
    "        save_path = os.path.join(save_directory, file_name) \n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "print(f\"Total epochs loaded: {total_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset depression (dataset real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metode Korelasi Pearson dataset healthy\n",
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "directory_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\6s_epochs_depression\"\n",
    "selected_electrodes_names = ['EEG FP1-R', 'EEG FP2-R', 'EEG C3-R', 'EEG C4-R', 'EEG O1-R', 'EEG O2-R', 'EEG T3-R', 'EEG T4-R']\n",
    "file_paths = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith(\".edf\")]\n",
    "save_directory = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\correlation_visualization_depression\"\n",
    "save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Depression\\csv_data_correlation_depression\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "os.makedirs(save_csv_path, exist_ok=True) \n",
    "total_epochs = 0\n",
    "for file_path in file_paths:\n",
    "    epochs = mne.read_epochs(file_path, preload=True)\n",
    "    num_epochs = len(epochs)\n",
    "    total_epochs += num_epochs\n",
    "    file_basename = os.path.basename(file_path).split('.')[0]\n",
    "    print(f\"{os.path.basename(file_path)}: {num_epochs} epochs\")\n",
    "\n",
    "    for epoch_idx, epoch_data in enumerate(epochs.get_data()):\n",
    "        selected_indices = [epochs.ch_names.index(name) for name in selected_electrodes_names if name in epochs.ch_names]\n",
    "        selected_data = epoch_data[selected_indices]\n",
    "        column_labels = [epochs.ch_names[idx] for idx in selected_indices]\n",
    "        df = pd.DataFrame(selected_data.T, columns=column_labels)\n",
    "        correlation_matrix = df.corr()\n",
    "        \n",
    "        csv_filename = f\"{file_basename}_correlation_matrix_epoch_{epoch_idx + 1}.csv\"\n",
    "        csv_file_path = os.path.join(save_csv_path, csv_filename)\n",
    "        correlation_matrix.to_csv(csv_file_path)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap=\"viridis\", fmt=\".2f\", \n",
    "                    xticklabels=column_labels, \n",
    "                    yticklabels=column_labels)\n",
    "        file_name = f\"{file_basename}_correlation_matrix_epoch_{epoch_idx + 1}.png\"\n",
    "        plt.title(f\"Pearson Correlation Matrix\")\n",
    "        save_path = os.path.join(save_directory, file_name) \n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "print(f\"Total epochs loaded: {total_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Matrix for Healthy Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_fif_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.fif')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "       r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_FT7_healthy\\alpha_band_FT7_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_T7_healthy\\alpha_band_T7_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_TP7_healthy\\alpha_band_TP7_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_TP8_healthy\\alpha_band_TP8_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_T8_healthy\\alpha_band_T8_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_FT8_healthy\\alpha_band_FT8_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_F3_healthy\\alpha_band_F3_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_F4_healthy\\alpha_band_F4_healthy\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['FT7', 'T7', 'TP7', 'TP8', 'T8', 'FT8', 'F3', 'F4']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_fif_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .fif files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\coherence_visualization_healthy\\alpha_band_coherence_healthy\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\csv_data_coherence_healthy\\alpha_band_csv_healthy\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=8, fmax=12, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "            \n",
    "            csv_file_name = f\"coherence_matrix_Healthy_alphaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Alpha Band Healthy')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Healthy_alphaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha band (dataset real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_edf_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.edf')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_C3-R_healthy\\alpha_band_C3-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_C4-R_healthy\\alpha_band_C4-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_FP1-R_healthy\\alpha_band_FP1-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_FP2-R_healthy\\alpha_band_FP2-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_O1-R_healthy\\alpha_band_O1-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_O2-R_healthy\\alpha_band_O2-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_T3-R_healthy\\alpha_band_T3-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_T4-R_healthy\\alpha_band_T4-R_healthy\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['EEG FP1-R', 'EEG FP2-R', 'EEG C3-R', 'EEG C4-R', 'EEG O1-R', 'EEG O2-R', 'EEG T3-R', 'EEG T4-R']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_edf_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .edf files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\coherence_visualization_healthy\\alpha_band_coherence_healthy\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\csv_data_coherence_healthy\\alpha_band_csv_healthy\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=8, fmax=12, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "            \n",
    "            csv_file_name = f\"coherence_matrix_Healthy_alphaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Alpha Band Healthy')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Healthy_alphaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_fif_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.fif')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "       r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_FT7_healthy\\beta_band_FT7_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_T7_healthy\\beta_band_T7_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_TP7_healthy\\beta_band_TP7_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_TP8_healthy\\beta_band_TP8_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_T8_healthy\\beta_band_T8_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_FT8_healthy\\beta_band_FT8_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_F3_healthy\\beta_band_F3_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_F4_healthy\\beta_band_F4_healthy\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['FT7', 'T7', 'TP7', 'TP8', 'T8', 'FT8', 'F3', 'F4']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_fif_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .fif files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\coherence_visualization_healthy\\beta_band_coherence_healthy\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\csv_data_coherence_healthy\\beta_band_csv_healthy\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=12, fmax=30, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "            \n",
    "            csv_file_name = f\"coherence_matrix_Healthy_thetaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Beta Band Healthy')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Healthy_betaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta band (dataset real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_edf_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.edf')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_C3-R_healthy\\beta_band_C3-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_C4-R_healthy\\beta_band_C4-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_FP1-R_healthy\\beta_band_FP1-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_FP2-R_healthy\\beta_band_FP2-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_O1-R_healthy\\beta_band_O1-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_O2-R_healthy\\beta_band_O2-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_T3-R_healthy\\beta_band_T3-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_T4-R_healthy\\beta_band_T4-R_healthy\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['EEG FP1-R', 'EEG FP2-R', 'EEG C3-R', 'EEG C4-R', 'EEG O1-R', 'EEG O2-R', 'EEG T3-R', 'EEG T4-R']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_edf_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .edf files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\coherence_visualization_healthy\\beta_band_coherence_healthy\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\csv_data_coherence_healthy\\beta_band_csv_healthy\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=12, fmax=30, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "            \n",
    "            csv_file_name = f\"coherence_matrix_Healthy_thetaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Beta Band Healthy')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Healthy_betaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_fif_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.fif')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "       r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_FT7_healthy\\delta_band_FT7_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_T7_healthy\\delta_band_T7_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_TP7_healthy\\delta_band_TP7_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_TP8_healthy\\delta_band_TP8_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_T8_healthy\\delta_band_T8_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_FT8_healthy\\delta_band_FT8_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_F3_healthy\\delta_band_F3_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_F4_healthy\\delta_band_F4_healthy\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['FT7', 'T7', 'TP7', 'TP8', 'T8', 'FT8', 'F3', 'F4']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_fif_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .fif files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\coherence_visualization_healthy\\delta_band_coherence_healthy\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\csv_data_coherence_healthy\\delta_band_csv_healthy\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=0.1, fmax=3, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "\n",
    "            csv_file_name = f\"coherence_matrix_Healthy_deltaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Delta Band Healthy')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Healthy_deltaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta band (dataset real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_edf_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.edf')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "       r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_C3-R_healthy\\delta_band_C3-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_C4-R_healthy\\delta_band_C4-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_FP1-R_healthy\\delta_band_FP1-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_FP2-R_healthy\\delta_band_FP2-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_O1-R_healthy\\delta_band_O1-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_O2-R_healthy\\delta_band_O2-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_T3-R_healthy\\delta_band_T3-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_T4-R_healthy\\delta_band_T4-R_healthy\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['EEG FP1-R', 'EEG FP2-R', 'EEG C3-R', 'EEG C4-R', 'EEG O1-R', 'EEG O2-R', 'EEG T3-R', 'EEG T4-R']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_edf_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .edf files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\coherence_visualization_healthy\\delta_band_coherence_healthy\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\csv_data_coherence_healthy\\delta_band_csv_healthy\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=0.1, fmax=3, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "\n",
    "            csv_file_name = f\"coherence_matrix_Healthy_deltaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Delta Band Healthy')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Healthy_deltaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_fif_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.fif')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_FT7_healthy\\gamma_band_FT7_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_T7_healthy\\gamma_band_T7_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_TP7_healthy\\gamma_band_TP7_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_TP8_healthy\\gamma_band_TP8_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_T8_healthy\\gamma_band_T8_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_FT8_healthy\\gamma_band_FT8_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_F3_healthy\\gamma_band_F3_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_F4_healthy\\gamma_band_F4_healthy\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['FT7', 'T7', 'TP7', 'TP8', 'T8', 'FT8', 'F3', 'F4']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_fif_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .fif files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\coherence_visualization_healthy\\gamma_band_coherence_healthy\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\csv_data_coherence_healthy\\gamma_band_csv_healthy\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=30, fmax=45, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "\n",
    "            csv_file_name = f\"coherence_matrix_Healthy_gammaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Gamma Band Healthy')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Healthy_gammaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma band (dataset real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_edf_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.edf')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_C3-R_healthy\\gamma_band_C3-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_C4-R_healthy\\gamma_band_C4-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_FP1-R_healthy\\gamma_band_FP1-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_FP2-R_healthy\\gamma_band_FP2-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_O1-R_healthy\\gamma_band_O1-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_O2-R_healthy\\gamma_band_O2-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_T3-R_healthy\\gamma_band_T3-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_T4-R_healthy\\gamma_band_T4-R_healthy\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['EEG FP1-R', 'EEG FP2-R', 'EEG C3-R', 'EEG C4-R', 'EEG O1-R', 'EEG O2-R', 'EEG T3-R', 'EEG T4-R']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_edf_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .edf files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\coherence_visualization_healthy\\gamma_band_coherence_healthy\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\csv_data_coherence_healthy\\gamma_band_csv_healthy\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=30, fmax=45, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "\n",
    "            csv_file_name = f\"coherence_matrix_Healthy_gammaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Gamma Band Healthy')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Healthy_gammaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theta band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_fif_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.fif')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_FT7_healthy\\theta_band_FT7_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_T7_healthy\\theta_band_T7_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_TP7_healthy\\theta_band_TP7_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_TP8_healthy\\theta_band_TP8_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_T8_healthy\\theta_band_T8_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_FT8_healthy\\theta_band_FT8_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_F3_healthy\\theta_band_F3_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\filter_channel_F4_healthy\\theta_band_F4_healthy\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['FT7', 'T7', 'TP7', 'TP8', 'T8', 'FT8', 'F3', 'F4']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_fif_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .fif files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\coherence_visualization_healthy\\theta_band_coherence_healthy\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\csv_data_coherence_healthy\\theta_band_csv_healthy\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=4, fmax=8, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "\n",
    "            csv_file_name = f\"coherence_matrix_Healthy_thetaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Theta Band Healthy')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Healthy_thetaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theta band (dataset real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "def find_edf_files(directory):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.edf')]\n",
    "\n",
    "channel_groups = [\n",
    "    [\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_C3-R_healthy\\theta_band_C3-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_C4-R_healthy\\theta_band_C4-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_FP1-R_healthy\\theta_band_FP1-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_FP2-R_healthy\\theta_band_FP2-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_O1-R_healthy\\theta_band_O1-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_O2-R_healthy\\theta_band_O2-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_T3-R_healthy\\theta_band_T3-R_healthy\",\n",
    "        r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\filter_channel_T4-R_healthy\\theta_band_T4-R_healthy\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "for channel_group in channel_groups:\n",
    "    all_epochs_data = []  # This will hold the data for all epochs across all channels\n",
    "    ch_names = ['EEG FP1-R', 'EEG FP2-R', 'EEG C3-R', 'EEG C4-R', 'EEG O1-R', 'EEG O2-R', 'EEG T3-R', 'EEG T4-R']\n",
    "    info = None  # This will be used to store the info structure for creating combined epochs later\n",
    "    for channel_dir in channel_group:\n",
    "        channel_epochs_data = []  # This will hold data for all epochs for the current channel\n",
    "        try:\n",
    "            file_paths = find_edf_files(channel_dir)\n",
    "            if not file_paths:\n",
    "                print(f\"No .edf files found in {channel_dir}. Skipping this directory.\")\n",
    "                continue\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    epochs = mne.read_epochs(file_path, preload=True)\n",
    "                    print(f\"{file_path} contains {len(epochs)} epochs\")\n",
    "                    if info is None:\n",
    "                        info = epochs.info  # We only need to set this once\n",
    "                    channel_epochs_data.append(epochs.get_data())  # Append data for these epochs\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read epochs from {file_path}. Error: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {channel_dir}: {e}\")\n",
    "            continue\n",
    "        if channel_epochs_data:\n",
    "            # Concatenate all epoch data for the current channel\n",
    "            channel_basename = os.path.basename(channel_dir)\n",
    "            total_epochs = sum(len(e) for e in channel_epochs_data)\n",
    "            print(f\"Channel {channel_basename} combined total epochs: {total_epochs}\")\n",
    "            channel_combined_data = np.concatenate(channel_epochs_data, axis=0)\n",
    "            all_epochs_data.append(channel_combined_data)\n",
    "\n",
    "    if all_epochs_data:\n",
    "        # Combine data from all channels, adjusting axis as necessary\n",
    "        data_combined = np.concatenate(all_epochs_data, axis=1)  # Adjust axis based on your data structure\n",
    "        print(f\"Total combined epochs across all channels: {data_combined.shape[0]}\")\n",
    "        print(f\"Diagnostic: Combined data shape: {data_combined.shape}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No epochs were loaded. Please check file paths and formats.\")\n",
    "        continue\n",
    "\n",
    "    # Now, create the combined epochs structure\n",
    "    if info is not None and data_combined.size > 0:\n",
    "        # mapped_ch_names = [electrode_name_mapping.get(ch_name, ch_name) for ch_name in ch_names]\n",
    "        # Update info to match the combined data\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=info['sfreq'], ch_types=['eeg'] * len(ch_names))\n",
    "        # Create a dummy events array and event_id dict, as they are required for creating EpochsArray\n",
    "        events = np.array([[i, 0, 1] for i in range(data_combined.shape[0])])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs_combined = mne.EpochsArray(data_combined, info, events=events, event_id=event_id)\n",
    "        print(f\"Starting connectivity analysis on combined data...\")\n",
    "        processed_epochs_count = 0\n",
    "        save_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\coherence_visualization_healthy\\theta_band_coherence_healthy\"\n",
    "        save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\csv_data_coherence_healthy\\theta_band_csv_healthy\"\n",
    "        for epoch_idx in range(len(epochs_combined)):\n",
    "            # Here you can proceed with your connectivity analysis on `epochs_combined`\n",
    "            con = spectral_connectivity_epochs(\n",
    "                epochs_combined[epoch_idx:epoch_idx+1], method='coh', mode='multitaper', sfreq=epochs_combined.info['sfreq'],\n",
    "                fmin=4, fmax=8, faverage=True, mt_adaptive=True, n_jobs=1\n",
    "            )\n",
    "            con_matrix = con.get_data(output='dense')[..., 0]\n",
    "            df_con = pd.DataFrame(con_matrix, index=ch_names, columns=ch_names)\n",
    "            processed_epochs_count += 1\n",
    "\n",
    "            csv_file_name = f\"coherence_matrix_Healthy_thetaBand_epoch_{epoch_idx + 1}.csv\"\n",
    "            csv_full_path = os.path.join(save_csv_path, csv_file_name)\n",
    "            df_con.to_csv(csv_full_path)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(df_con, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "            plt.title('Coherence Matrix for Theta Band Healthy')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.xlabel('Channels')\n",
    "            file_name = f\"coherence_matrix_Healthy_thetaBand_epoch_{epoch_idx + 1}.png\"\n",
    "            full_path = os.path.join(save_path, file_name)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close()\n",
    "            print(f\"Processed and visualized {processed_epochs_count} epochs so far.\")\n",
    "            print(f\"Total epochs processed for visualization: {processed_epochs_count}\")\n",
    "        else:\n",
    "            print(\"No combined data available for creating epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation on Healthy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metode Korelasi Pearson dataset healthy\n",
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "directory_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\6s_epochs_healthy\"\n",
    "selected_electrodes_names = ['FT7', 'T7', 'TP7', 'TP8', 'T8', 'FT8', 'F3', 'F4']\n",
    "file_paths = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith(\".fif\")]\n",
    "save_directory = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\correlation_visualization_healthy\"\n",
    "save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Healthy\\csv_data_correlation_healthy\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "total_epochs = 0\n",
    "for file_path in file_paths:\n",
    "    epochs = mne.read_epochs(file_path, preload=True)\n",
    "    num_epochs = len(epochs)\n",
    "    total_epochs += num_epochs\n",
    "    file_basename = os.path.basename(file_path).split('.')[0]\n",
    "    print(f\"{os.path.basename(file_path)}: {num_epochs} epochs\")\n",
    "\n",
    "    for epoch_idx, epoch_data in enumerate(epochs.get_data()):\n",
    "        selected_indices = [epochs.ch_names.index(name) for name in selected_electrodes_names if name in epochs.ch_names]\n",
    "        selected_data = epoch_data[selected_indices]\n",
    "        column_labels = [epochs.ch_names[idx] for idx in selected_indices]\n",
    "        df = pd.DataFrame(selected_data.T, columns=column_labels)\n",
    "        correlation_matrix = df.corr()\n",
    "        \n",
    "        csv_filename = f\"{file_basename}_correlation_matrix_epoch_{epoch_idx + 1}.csv\"\n",
    "        csv_file_path = os.path.join(save_csv_path, csv_filename)\n",
    "        correlation_matrix.to_csv(csv_file_path)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap=\"viridis\", fmt=\".2f\", \n",
    "                    xticklabels=column_labels, \n",
    "                    yticklabels=column_labels)\n",
    "        file_name = f\"{file_basename}_correlation_matrix_epoch_{epoch_idx + 1}.png\"\n",
    "        plt.title(f\"Pearson Correlation Matrix\")\n",
    "        save_path = os.path.join(save_directory, file_name) \n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "print(f\"Total epochs loaded: {total_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation on healthy dataset real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metode Korelasi Pearson dataset healthy\n",
    "# electrode_name_mapping = {\n",
    "#     'EEG039': 'FT7',\n",
    "#     'EEG115': 'FT8',\n",
    "#     'EEG045': 'T7',\n",
    "#     'EEG108': 'T8',\n",
    "#     'EEG050': 'TP7',\n",
    "#     'EEG101': 'TP8',\n",
    "#     'EEG024': 'F3',\n",
    "#     'EEG124': 'F4'\n",
    "# }\n",
    "\n",
    "directory_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\6s_epochs_healthy\"\n",
    "selected_electrodes_names = ['EEG FP1-R', 'EEG FP2-R', 'EEG C3-R', 'EEG C4-R', 'EEG O1-R', 'EEG O2-R', 'EEG T3-R', 'EEG T4-R']\n",
    "file_paths = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith(\".edf\")]\n",
    "save_directory = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\correlation_visualization_healthy\"\n",
    "save_csv_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Healthy\\csv_data_correlation_healthy\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "total_epochs = 0\n",
    "for file_path in file_paths:\n",
    "    epochs = mne.read_epochs(file_path, preload=True)\n",
    "    num_epochs = len(epochs)\n",
    "    total_epochs += num_epochs\n",
    "    file_basename = os.path.basename(file_path).split('.')[0]\n",
    "    print(f\"{os.path.basename(file_path)}: {num_epochs} epochs\")\n",
    "\n",
    "    for epoch_idx, epoch_data in enumerate(epochs.get_data()):\n",
    "        selected_indices = [epochs.ch_names.index(name) for name in selected_electrodes_names if name in epochs.ch_names]\n",
    "        selected_data = epoch_data[selected_indices]\n",
    "        column_labels = [epochs.ch_names[idx] for idx in selected_indices]\n",
    "        df = pd.DataFrame(selected_data.T, columns=column_labels)\n",
    "        correlation_matrix = df.corr()\n",
    "        \n",
    "        csv_filename = f\"{file_basename}_correlation_matrix_epoch_{epoch_idx + 1}.csv\"\n",
    "        csv_file_path = os.path.join(save_csv_path, csv_filename)\n",
    "        correlation_matrix.to_csv(csv_file_path)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap=\"viridis\", fmt=\".2f\", \n",
    "                    xticklabels=column_labels, \n",
    "                    yticklabels=column_labels)\n",
    "        file_name = f\"{file_basename}_correlation_matrix_epoch_{epoch_idx + 1}.png\"\n",
    "        plt.title(f\"Pearson Correlation Matrix\")\n",
    "        save_path = os.path.join(save_directory, file_name) \n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "print(f\"Total epochs loaded: {total_epochs}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
