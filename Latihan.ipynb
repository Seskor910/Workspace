{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler, Callback, TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from shutil import copyfile\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path data\n",
    "train_dir = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Training_dataset\\bahan_training\\train\"\n",
    "val_dir = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Training_dataset\\bahan_training\\val\"\n",
    "test_dir = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Training_dataset\\dataset_testing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Periksa label data\n",
    "def display_sample_images(class_dir, class_names, sample_size=5):\n",
    "    for class_name in class_names:\n",
    "        print(f\"Class: {class_name}\")\n",
    "        path = os.path.join(class_dir, class_name)\n",
    "        images = random.sample(os.listdir(path), sample_size)\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        for i, image in enumerate(images):\n",
    "            plt.subplot(1, sample_size, i + 1)\n",
    "            img = Image.open(os.path.join(path, image))\n",
    "            plt.imshow(img)\n",
    "            plt.title(class_name)\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "class_names = ['Depression', 'Healthy']\n",
    "display_sample_images(train_dir, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Karena dataset tidak balance jadi yang healthy 253 image\n",
    "def reduce_class_files(src_directory, num_files_to_remove):\n",
    "    files = [file for file in os.listdir(src_directory) if os.path.isfile(os.path.join(src_directory, file))]\n",
    "    \n",
    "    if num_files_to_remove > len(files):\n",
    "        raise ValueError(\"Number of files to remove exceeds the number of available files\")\n",
    "\n",
    "    files_to_remove = random.sample(files, num_files_to_remove)\n",
    "    \n",
    "    for file in files_to_remove:\n",
    "        os.remove(os.path.join(src_directory, file))\n",
    "        print(f\"Removed {file}\")\n",
    "\n",
    "Healthy_dir = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_5\\Training_dataset\\dataset_balanced\\Healthy\"\n",
    "reduce_class_files(Healthy_dir, 1168)\n",
    "\n",
    "# # Example usage for validation dataset\n",
    "# val_depression_dir = r'E:\\Vscode\\Tugas Anime\\Workspace\\Bahan_Training_V4\\val\\Healthy'\n",
    "# # Assume you want to remove 43 images from the validation dataset\n",
    "# reduce_class_files(val_depression_dir, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi ngesplit\n",
    "def copy_random_files_unique(source_folder: str, dest_folders: dict):\n",
    "    # List all files in the source folder\n",
    "    all_files = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n",
    "    \n",
    "    total_files_needed = sum(dest_folders.values())\n",
    "    if len(all_files) < total_files_needed:\n",
    "        raise ValueError(\"Not enough files in the source folder to fill all destination folders uniquely.\")\n",
    "    \n",
    "    # Shuffle files to randomize selection\n",
    "    random.shuffle(all_files)\n",
    "\n",
    "    used_files = set()\n",
    "\n",
    "    # Iterate over destination folders and their required file counts\n",
    "    for dest_folder, num_files in dest_folders.items():\n",
    "        # Create the folder if it doesn't exist\n",
    "        if not os.path.exists(dest_folder):\n",
    "            os.makedirs(dest_folder)\n",
    "        \n",
    "        selected_files = 0\n",
    "        file_index = 0\n",
    "        \n",
    "        # Select and copy files to the current destination folder\n",
    "        while selected_files < num_files:\n",
    "            file_name = all_files[file_index]\n",
    "            if file_name not in used_files:\n",
    "                shutil.copy(os.path.join(source_folder, file_name), dest_folder)\n",
    "                used_files.add(file_name)\n",
    "                selected_files += 1\n",
    "            file_index += 1  # Move to the next file\n",
    "            \n",
    "            # This shouldn't happen with earlier checks but adds robustness\n",
    "            if file_index >= len(all_files):\n",
    "                raise RuntimeError(\"Ran out of unique files to select from.\")\n",
    "\n",
    "# Example usage\n",
    "source_folder = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_5\\Training_dataset\\dataset_balanced\\Depression\"\n",
    "dest_folders = {\n",
    "    r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_5\\Training_dataset\\bahan_training\\train\\Depression\": 1139,\n",
    "    r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_5\\Training_dataset\\bahan_training\\val\\Depression\": 162,\n",
    "    r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_5\\Training_dataset\\dataset_testing\\Depression\": 325\n",
    "}\n",
    "\n",
    "copy_random_files_unique(source_folder, dest_folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi ngesplit\n",
    "def copy_random_files_unique(source_folder: str, dest_folders: dict):\n",
    "    # List all files in the source folder\n",
    "    all_files = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n",
    "    \n",
    "    total_files_needed = sum(dest_folders.values())\n",
    "    if len(all_files) < total_files_needed:\n",
    "        raise ValueError(\"Not enough files in the source folder to fill all destination folders uniquely.\")\n",
    "    \n",
    "    # Shuffle files to randomize selection\n",
    "    random.shuffle(all_files)\n",
    "\n",
    "    used_files = set()\n",
    "\n",
    "    # Iterate over destination folders and their required file counts\n",
    "    for dest_folder, num_files in dest_folders.items():\n",
    "        # Create the folder if it doesn't exist\n",
    "        if not os.path.exists(dest_folder):\n",
    "            os.makedirs(dest_folder)\n",
    "        \n",
    "        selected_files = 0\n",
    "        file_index = 0\n",
    "        \n",
    "        # Select and copy files to the current destination folder\n",
    "        while selected_files < num_files:\n",
    "            file_name = all_files[file_index]\n",
    "            if file_name not in used_files:\n",
    "                shutil.copy(os.path.join(source_folder, file_name), dest_folder)\n",
    "                used_files.add(file_name)\n",
    "                selected_files += 1\n",
    "            file_index += 1  # Move to the next file\n",
    "            \n",
    "            # This shouldn't happen with earlier checks but adds robustness\n",
    "            if file_index >= len(all_files):\n",
    "                raise RuntimeError(\"Ran out of unique files to select from.\")\n",
    "\n",
    "# Example usage\n",
    "source_folder = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_5\\Training_dataset\\dataset_balanced\\Healthy\"\n",
    "dest_folders = {\n",
    "    r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_5\\Training_dataset\\bahan_training\\train\\Healthy\": 1139,\n",
    "    r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_5\\Training_dataset\\bahan_training\\val\\Healthy\": 162,\n",
    "    r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_5\\Training_dataset\\dataset_testing\\Healthy\": 325\n",
    "}\n",
    "\n",
    "copy_random_files_unique(source_folder, dest_folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Periksa ketidakseimbangan data\n",
    "def count_samples_per_class(class_dir, class_names):\n",
    "    counts = {}\n",
    "    for class_name in class_names:\n",
    "        path = os.path.join(class_dir, class_name)\n",
    "        counts[class_name] = len(os.listdir(path))\n",
    "    return counts\n",
    "\n",
    "# Check training data balance\n",
    "train_counts = count_samples_per_class(train_dir, class_names)\n",
    "print(\"Training Data:\", train_counts)\n",
    "\n",
    "# Check validation data balance\n",
    "val_counts = count_samples_per_class(val_dir, class_names)\n",
    "print(\"Validation Data:\", val_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "validation_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    classes=['Depression', 'Healthy'],\n",
    "    target_size=(16, 24),\n",
    "    batch_size=4,\n",
    "    # class_mode='binary'\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    classes=['Depression', 'Healthy'],\n",
    "    target_size=(16, 24),\n",
    "    batch_size=4,\n",
    "    # class_mode='binary',\n",
    "    class_mode='categorical',\n",
    "    # shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    classes = ['Depression', 'Healthy'],\n",
    "    target_size=(16, 24),\n",
    "    batch_size=4,\n",
    "    # class_mode='binary',\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# pembuatan model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        # Layer 1\n",
    "        Conv2D(32, (3, 3), strides=(1, 1), activation='relu', input_shape=(16, 24, 3)),\n",
    "        # Dropout(0.2),\n",
    "        Dropout(0.1),\n",
    "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        # Layer 2\n",
    "        Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n",
    "        # Dropout(0.5),\n",
    "        Dropout(0.1),\n",
    "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        # Flatten\n",
    "        Flatten(),\n",
    "        # Fully connected layer\n",
    "        Dense(32, activation='relu'),\n",
    "        # Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        # Dropout(0.5),\n",
    "        Dropout(0.1),\n",
    "        # Dense(1, activation='sigmoid')\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    adam_optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=adam_optimizer,\n",
    "                # loss='binary_crossentropy',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'AUC'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "# # Save the model architecture as an image file\n",
    "# plot_model(model, to_file=r\"E:\\Vscode\\Tugas Anime\\Workspace\\model_architecture1.png\", show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# latihan\n",
    "history = model.fit(train_generator,\n",
    "                    # steps_per_epoch=len(train_generator),\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_data = validation_generator,\n",
    "                    # validation_steps=len(validation_generator)\n",
    "                    )\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "test_loss, test_acc, test_auc = model.evaluate(test_generator, steps=len(test_generator))\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test AUC: {test_auc:.2f}\")\n",
    "\n",
    "# Predicting on the entire test set\n",
    "predictions = model.predict(test_generator, steps=len(test_generator))\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity:.2f}\")\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ridho_exe5_model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "validation_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    classes=['Depression', 'Healthy'],\n",
    "    target_size=(16, 24),\n",
    "    batch_size=4,\n",
    "    # class_mode='binary'\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    classes=['Depression', 'Healthy'],\n",
    "    target_size=(16, 24),\n",
    "    batch_size=4,\n",
    "    # class_mode='binary',\n",
    "    class_mode='categorical',\n",
    "    # shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    classes = ['Depression', 'Healthy'],\n",
    "    target_size=(16, 24),\n",
    "    batch_size=4,\n",
    "    # class_mode='binary',\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# pembuatan model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        # Layer 1\n",
    "        Conv2D(32, (3, 3), strides=(1, 1), activation='relu', input_shape=(16, 24, 3)),\n",
    "        # Dropout(0.2),\n",
    "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        # Layer 2\n",
    "        Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n",
    "        # Dropout(0.5),\n",
    "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        # Flatten\n",
    "        Flatten(),\n",
    "        # Fully connected layer\n",
    "        Dense(32, activation='relu'),\n",
    "        # Dropout(0.5),\n",
    "        # Dense(1, activation='sigmoid')\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    adam_optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=adam_optimizer,\n",
    "                # loss='binary_crossentropy',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'AUC'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "# latihan\n",
    "history = model.fit(train_generator,\n",
    "                    # steps_per_epoch=len(train_generator),\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_data = validation_generator,\n",
    "                    # validation_steps=len(validation_generator)\n",
    "                    )\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "test_loss, test_acc, test_auc = model.evaluate(test_generator, steps=len(test_generator))\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test AUC: {test_auc:.2f}\")\n",
    "\n",
    "# Predicting on the entire test set\n",
    "predictions = model.predict(test_generator, steps=len(test_generator))\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity:.2f}\")\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ridho_exe2_model2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "# train_datagen = ImageDataGenerator()\n",
    "# validation_datagen = ImageDataGenerator()\n",
    "# test_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    classes=['Depression', 'Healthy'],\n",
    "    target_size=(16, 24),\n",
    "    batch_size=4,\n",
    "    # class_mode='binary'\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    classes=['Depression', 'Healthy'],\n",
    "    target_size=(16, 24),\n",
    "    batch_size=4,\n",
    "    # class_mode='binary',\n",
    "    class_mode='categorical',\n",
    "    # shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    classes = ['Depression', 'Healthy'],\n",
    "    target_size=(16, 24),\n",
    "    batch_size=4,\n",
    "    # class_mode='binary',\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# pembuatan model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        # Layer 1\n",
    "        Conv2D(32, (3, 3), strides=(1, 1), activation='relu', input_shape=(16, 24, 3)),\n",
    "        # Dropout(0.2),\n",
    "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        # Layer 2\n",
    "        Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n",
    "        # Dropout(0.5),\n",
    "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        # Flatten\n",
    "        Flatten(),\n",
    "        # Fully connected layer\n",
    "        Dense(32, activation='relu'),\n",
    "        # Dropout(0.5),\n",
    "        # Dense(1, activation='sigmoid')\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    adam_optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=adam_optimizer,\n",
    "                # loss='binary_crossentropy',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'AUC'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "# latihan\n",
    "history = model.fit(train_generator,\n",
    "                    # steps_per_epoch=len(train_generator),\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_data = validation_generator,\n",
    "                    # validation_steps=len(validation_generator)\n",
    "                    )\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "# test_loss, test_acc = model.evaluate(test_generator, steps=len(test_generator))\n",
    "test_loss, test_acc, test_auc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test AUC: {test_auc:.2f}\")\n",
    "\n",
    "# Predicting on the entire test set\n",
    "# predictions = model.predict(test_generator, steps=len(test_generator))\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity:.2f}\")\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ridho_exe5_model3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load your pre-trained model\n",
    "model_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\ridho_exe2_model2.h5\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Directory containing images for prediction\n",
    "healthy_dir = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_2\\Sumber_data\\Training_dataset\\dataset_testing\\Healthy\"\n",
    "\n",
    "# Dictionary to store the count of predicted classes\n",
    "class_count = {'Depression': 0, 'Healthy': 0}\n",
    "\n",
    "# Dictionary to store filenames of predicted classes\n",
    "predicted_files = {'Depression': [], 'Healthy': []}\n",
    "\n",
    "\n",
    "# Function to process the directory\n",
    "def process_directory(directory):\n",
    "    for file in os.listdir(directory):\n",
    "        # Ensure the file is an image\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(directory, file)\n",
    "            img = load_img(img_path, target_size=(16, 24))\n",
    "            img_array = img_to_array(img) /255.0\n",
    "            img_array = np.expand_dims(img_array, axis=0)  # Shape (1, 16, 24, 3)\n",
    "\n",
    "            predictions = model.predict(img_array)\n",
    "            predicted_class_index = np.argmax(predictions, axis=1)\n",
    "            predicted_class = class_labels[predicted_class_index[0]]\n",
    "            \n",
    "            # Increment count of the predicted class\n",
    "            class_count[predicted_class] += 1\n",
    "            predicted_files[predicted_class].append(file)\n",
    "\n",
    "            # Optionally print out the file and its predicted class\n",
    "            print(f\"{file}: Predicted as {predicted_class}\")\n",
    "\n",
    "\n",
    "\n",
    "# Class labels mapping\n",
    "class_labels = {0: 'Depression', 1: 'Healthy'}\n",
    "\n",
    "# Process the directory\n",
    "process_directory(healthy_dir)\n",
    "\n",
    "# Output the counts\n",
    "print(\"Classification counts:\", class_count)\n",
    "\n",
    "# Output the filenames of each predicted class\n",
    "print(\"Files predicted as Depression:\", predicted_files['Depression'])\n",
    "print(\"Files predicted as Healthy:\", predicted_files['Healthy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load your pre-trained model\n",
    "model_path = r\"E:\\Vscode\\Tugas Anime\\Workspace\\Eksperimen_4\\Training_dataset\\hasil_training\\model_3\\ridho_exe4_model3.h5\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Directory containing images for prediction\n",
    "depression_dir = r\"Eksperimen_4\\Training_dataset\\dataset_testing\\Depression\"\n",
    "\n",
    "# Dictionary to store the count of predicted classes\n",
    "class_count = {'Depression': 0, 'Healthy': 0}\n",
    "\n",
    "# Dictionary to store filenames of predicted classes\n",
    "predicted_files = {'Depression': [], 'Healthy': []}\n",
    "\n",
    "# Function to process the directory\n",
    "def process_directory(directory):\n",
    "    for file in os.listdir(directory):\n",
    "        # Ensure the file is an image\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(directory, file)\n",
    "            img = load_img(img_path, target_size=(16, 24))\n",
    "            img_array = img_to_array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=0)  # Shape (1, 16, 24, 3)\n",
    "\n",
    "            predictions = model.predict(img_array)\n",
    "            predicted_class_index = np.argmax(predictions, axis=1)\n",
    "            predicted_class = class_labels[predicted_class_index[0]]\n",
    "            \n",
    "            # Increment count of the predicted class\n",
    "            class_count[predicted_class] += 1\n",
    "            predicted_files[predicted_class].append(file)\n",
    "\n",
    "            # Optionally print out the file and its predicted class\n",
    "            print(f\"{file}: Predicted as {predicted_class}\")\n",
    "\n",
    "# Class labels mapping\n",
    "class_labels = {0: 'Depression', 1: 'Healthy'}\n",
    "\n",
    "# Process the directory\n",
    "process_directory(depression_dir)\n",
    "\n",
    "# Output the counts\n",
    "print(\"Classification counts:\", class_count)\n",
    "\n",
    "# Output the filenames of each predicted class\n",
    "print(\"Files predicted as Depression:\", predicted_files['Depression'])\n",
    "print(\"Files predicted as Healthy:\", predicted_files['Healthy'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
